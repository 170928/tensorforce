{
  "log_level": "info",
  "batch_size": 128,
  "discount": 0.99,
  "baseline": {
    "type": "mlp",
    "size": 32,
    "epochs": 5,
    "update_batch_size" : 32
  },
  "gae_rewards": true,
  "gae_lambda": 0.95,
  "normalize_rewards" : true,
  "learning_rate" : 0.0005,
  "entropy_penalty" :  0.01,
  "epochs" : 10,
  "optimizer_batch_size" : 32,
  "loss_clipping" : 0.2
}
